{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.set_printoptions(linewidth=200)\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will go over the technique of modifying the attention mask when pre-training transformer models to ensure attention is only paid to tokens relevant to the current sequence text while at the same time leveraging the full context window of the model through packing sequences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume we would like to process the following three sentences in a single forward pass to make full use of the available GPU resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The cat sat on the mat\"\n",
    "sentence2 = \"The dog ate my homework\"\n",
    "sentence3 = \"My aunt is a teacher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply concatenate the tokenized sentences and using either an <bos> or <eos> token, the model will know when a new sentence starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat<|endoftext|>The dog ate my homework<|endoftext|>My aunt is a teacher<|endoftext|>'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence1, sentence2, sentence3]\n",
    "tokenized_sentences = tokenizer(sentences, return_attention_mask=False, add_special_tokens=False)[\"input_ids\"]\n",
    "tokenized_sentences = [t for s in tokenized_sentences for t in s + [tokenizer.eos_token_id]]\n",
    "tokenizer.decode(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard attention mask for causal language modeling for the packed sequences would look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = torch.tensor(tokenized_sentences)\n",
    "attn_mask = torch.ones(tokenized_sentences.size(0), tokenized_sentences.size(0), dtype=torch.bool).tril()\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this mask however, when processing the second sentence, the model can still attend to tokens in the first sentence which is not ideal as the two examples are independent. To fix this we can truncate the attention mask in a certain way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When having only one sample in the batch it is relatively easy to do in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True, False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,  True, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True,  True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attention_mask_for_packed_sequence(x, token_id, eos: bool = True):\n",
    "    # store sequence length in variable for easier readability\n",
    "    T = tokenized_sentences.size(0)\n",
    "    # get indices of all EOS tokens\n",
    "    eos_indices = (tokenized_sentences == tokenizer.eos_token_id).nonzero().squeeze()\n",
    "    # from indices, get length of each sequence\n",
    "    reps = torch.cat([eos_indices[[0]]+1, eos_indices[1:] - eos_indices[:-1]])\n",
    "    # repeat each eos index n times along dimension 1 (n is the number of tokens in the sequence)\n",
    "    repeated_idx = torch.repeat_interleave(eos_indices, reps).view(1,-1).expand(T, -1)\n",
    "    # create tensor with all indices from 0 to T-1 repeated T times along dimesion 1\n",
    "    mask_indices = torch.arange(T).view(-1,1).expand(-1, T)\n",
    "    # create causal mask and additionally mask out all tokens from preceeding sequences\n",
    "    mask = torch.ones(T, T, dtype=torch.bool).tril().expand(-1, -1)\n",
    "    mask.masked_fill_(mask_indices > repeated_idx, False)\n",
    "    return mask\n",
    "\n",
    "get_attention_mask_for_packed_sequence(tokenized_sentences, tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when having a batch of packed sequences it is a little bit more challenging due to the additional dimension. Lets create a second item of packed sqeuences to get a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4 = \"Rome wasn't built in a day\"\n",
    "sentence5 = \"My hovercraft is full of eels\"\n",
    "\n",
    "sentences = [sentence4, sentence5]\n",
    "tokenized_sentences2 = tokenizer(sentences, return_attention_mask=False, add_special_tokens=False)[\"input_ids\"]\n",
    "tokenized_sentences2 = torch.tensor([t for s in tokenized_sentences2 for t in s + [tokenizer.eos_token_id]])\n",
    "\n",
    "batch = torch.nn.utils.rnn.pad_sequence([tokenized_sentences, tokenized_sentences2], batch_first=True, padding_value=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go over the solution step by step. First lets assign the shape of the batch to two variables B and T. This makes the following code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T = batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct a tensor like \"repated_index\" tensor in the example from above. For this we need the indices of the eos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 13, 19, 28, 37, 38])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_idx = (batch.view(-1) == tokenizer.eos_token_id).nonzero(as_tuple=True)[0] + 1\n",
    "eos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this index vector we add the 0 index and the last token index for each batch item. This is needed to be able to separate the batch items again later on. We then remove duplicates (in case the first or last index for a batch item is already present) and sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  7, 13, 19, 28, 37, 38])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_idx_expanded = torch.cat([eos_idx, torch.arange(0,B*T+1,T)]).unique().sort()[0]\n",
    "eos_idx_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next since our index vector contains the global indices of eos tokens within the batch (e.g. the forst index of the second batch item = T) we need to normalize the indices by the sequence length. For the normalized indices we replace zeros with T. This is needed in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19,  7, 13, 19,  9, 18, 19])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_idx = eos_idx_expanded - (eos_idx_expanded // T) * T\n",
    "normalized_idx = torch.where(normalized_idx == 0, T, normalized_idx)\n",
    "normalized_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the normalized indices we can check how often we need to repeat each EOS token index to get the correct sequence length. To achieve this we needed to have the last index for each sequence present. If we didnt replace 0s with T in the step beforfe the number of repetitions for the last eos index in each batch would be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 6, 9, 9, 1])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps = normalized_idx[1:] - normalized_idx[:-1]\n",
    "reps = torch.where(reps < 1, normalized_idx[1:], reps)\n",
    "reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the batched repeated index tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19],\n",
       "        [ 7,  7,  7,  7,  7,  7,  7, 13, 13, 13, 13, 13, 13, 19, 19, 19, 19, 19, 19]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_idx = torch.repeat_interleave(normalized_idx[1:], reps).view(B,1,T).expand(-1,T,-1)\n",
    "repeated_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is similar to the example with batch size = 1. We construct a tensor with indices from 0 to T-1 repeated T times along dimension 1 and create a causal mask. We then mask out all tokens from preceeding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices = torch.arange(T).view(1,-1,1).expand(B, -1, T)\n",
    "# create mask\n",
    "mask = torch.ones(T, T, dtype=torch.bool).tril().expand(B, -1, -1)\n",
    "mask = mask.masked_fill(mask_indices >= repeated_idx, False)\n",
    "mask[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full function. I added to possiblity to chose between checking eos tokens or bos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_mask_for_packed_sequence(x, token_id, eos: bool = True):\n",
    "    B, T = x.shape\n",
    "    eos_idx = (x.view(-1) == token_id).nonzero(as_tuple=True)[0] + eos\n",
    "    eos_idx_expanded = torch.cat([eos_idx, torch.arange(0,B*T+1,T)]).unique().sort()[0]\n",
    "    normalized_idx = eos_idx_expanded - (eos_idx_expanded // T) * T\n",
    "    normalized_idx = torch.where(normalized_idx == 0, T, normalized_idx)\n",
    "    reps = normalized_idx[1:] - normalized_idx[:-1]\n",
    "    reps = torch.where(reps < 1, normalized_idx[1:], reps)\n",
    "    repeated_idx = torch.repeat_interleave(normalized_idx[1:], reps).view(B,1,T).expand(-1,T,-1)\n",
    "    mask_indices = torch.arange(T).view(1,-1,1).expand(B, -1, T)\n",
    "    mask = torch.ones(T, T, dtype=torch.bool).tril().expand(B, -1, -1)\n",
    "    mask = mask.masked_fill(mask_indices >= repeated_idx, False)\n",
    "    return mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
